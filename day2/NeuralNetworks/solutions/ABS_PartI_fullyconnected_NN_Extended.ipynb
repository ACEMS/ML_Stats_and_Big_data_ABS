{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### Import the data for MNIST, this will be downloaded if\n",
    "### necessary or copy it over manually into the directory\n",
    "### MNIST_data`\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Import the tensorflow module and create a session\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "### A helper function to randomly initialise weights\n",
    "def my_weights(n_inputs, n_outputs, name='default_weights'):\n",
    "  return tf.Variable(tf.truncated_normal([n_inputs, n_outputs],\n",
    "               stddev=1.0/math.sqrt(float(n_inputs))), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### x is for the input y_ holds the output\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) # for the input data 28*28 image as a vector\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10]) # output layer has 10 classes\n",
    "\n",
    "###\n",
    "n_1 = 600\n",
    "n_2 = 200\n",
    "n_c = 10\n",
    "\n",
    "### Layer 1.\n",
    "W_1 = my_weights(784, n_1, name='W_layer1')\n",
    "b_1 = tf.Variable(tf.zeros([n_1]))\n",
    "# Connect it up\n",
    "layer1 = tf.nn.relu(tf.matmul(x,W_1) + b_1)\n",
    "\n",
    "### Layer 2.\n",
    "W_2 = my_weights(n_1, n_2, name='W_layer2')\n",
    "b_2 = tf.Variable(tf.zeros([n_2]))\n",
    "# Connect it up\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W_2) + b_2)\n",
    "\n",
    "### Output layer\n",
    "W_out = my_weights(n_2, n_c, name='W_output')\n",
    "b_out = tf.Variable(tf.zeros([n_c]))\n",
    "# Connect it up\n",
    "y = tf.matmul(layer2, W_out) + b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The cross entropy loss\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "### The training algorithm, gradient descent based on cross entropy\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Work out the accuracy by using y_ as the estimated label and y as \n",
    "### the true label (GT).\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Initialise the variables of the graph e.g. x, y_, W, b\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.49\n",
      "step 100, training accuracy 0.98\n",
      "step 200, training accuracy 0.99\n",
      "step 300, training accuracy 0.97\n",
      "step 400, training accuracy 0.99\n",
      "step 500, training accuracy 1\n",
      "step 600, training accuracy 0.99\n",
      "step 700, training accuracy 1\n",
      "step 800, training accuracy 1\n",
      "step 900, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "### Go through and do 10 iterations of training, using a batch of 100 samples\n",
    "for i in range(1000):\n",
    "  # Randomly select a batch of 100 samples\n",
    "  batch = mnist.train.next_batch(100)\n",
    "    \n",
    "  # Take the images and x is equal to them, y_ is equal to the labels\n",
    "  _, xentropy = sess.run([train_step, cross_entropy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "  # Provide debug about every 100 iterations\n",
    "  if i % 100 == 0:\n",
    "    train_accuracy = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "    print('step %d, training accuracy %g' % (i, train_accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9701\n"
     ]
    }
   ],
   "source": [
    "### provide all the test images as the input x and all the test labels\n",
    "### as y_\n",
    "print(sess.run(accuracy,feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
